# examples/libero/test_paligemma_hf.py
import dataclasses
import logging
import pathlib
import numpy as np
import tyro
import pickle
from PIL import Image
import torch
from transformers import PaliGemmaForConditionalGeneration, PaliGemmaProcessor
import time

@dataclasses.dataclass
class HFInferArgs:
    """HuggingFace æ¨ç†é…ç½®"""
    data_file: str = "libero_observation_data/libero_observations.pkl"
    output_dir: str = "paligemma_hf_results"
    
    # æ¨¡å‹é…ç½®
    model_name: str = "/home/zhiyu/mzh/openpi/checkpoints/paligemma_hf"
    use_gpu: bool = True
    max_new_tokens: int = 50
    
    # ç”Ÿæˆé…ç½®
    do_sample: bool = False  # è´ªå¿ƒè§£ç 
    temperature: float = 1.0
    top_k: int = 50
    top_p: float = 0.9

class HFPaliGemmaInferencer:
    """HuggingFace PaliGemma æ¨ç†å™¨"""
    
    def __init__(self, model_name: str, use_gpu: bool = True):
        print(f"=== åˆå§‹åŒ– HuggingFace PaliGemma ===")
        print(f"æ¨¡å‹: {model_name}")
        
        # è®¾å¤‡é…ç½®
        self.device = "cuda" if use_gpu and torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        if torch.cuda.is_available() and use_gpu:
            print(f"CUDA è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
            print(f"å½“å‰ CUDA è®¾å¤‡: {torch.cuda.current_device()}")
            print(f"è®¾å¤‡åç§°: {torch.cuda.get_device_name()}")
            print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        
        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
        print("åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨...")
        try:
            self.processor = PaliGemmaProcessor.from_pretrained(model_name)
            self.model = PaliGemmaForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if use_gpu else torch.float32,
                device_map="auto" if use_gpu else None
            )
            
            if not use_gpu:
                self.model = self.model.to(self.device)
            
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            print(f"   - å¤„ç†å™¨è¯æ±‡è¡¨å¤§å°: {len(self.processor.tokenizer)}")
            print(f"   - æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()) / 1e9:.2f}B")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def generate_caption(self, image: np.ndarray, prompt: str = "caption en:", **kwargs) -> str:
        """ç”Ÿæˆå›¾åƒæè¿°"""
        
        try:
            print(f"\n--- ç”Ÿæˆæè¿° ---")
            print(f"Prompt: '{prompt}'")
            print(f"å›¾åƒå½¢çŠ¶: {image.shape}")
            
            # è½¬æ¢å›¾åƒæ ¼å¼
            if isinstance(image, np.ndarray):
                if image.dtype != np.uint8:
                    # å‡è®¾è¾“å…¥æ˜¯ [0, 1] èŒƒå›´çš„ float
                    image = (image * 255).astype(np.uint8)
                image = Image.fromarray(image)
            
            print(f"PIL å›¾åƒå¤§å°: {image.size}")
            
            # å¤„ç†è¾“å…¥
            start_time = time.time()
            inputs = self.processor(
                text=prompt, 
                images=image, 
                return_tensors="pt"
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            process_time = time.time() - start_time
            print(f"è¾“å…¥å¤„ç†æ—¶é—´: {process_time:.3f}s")
            
            # æ‰“å°è¾“å…¥ä¿¡æ¯
            print(f"è¾“å…¥ tokens å½¢çŠ¶: {inputs['input_ids'].shape}")
            print(f"è¾“å…¥ tokens: {inputs['input_ids'][0].tolist()}")
            
            # è§£ç è¾“å…¥ä»¥éªŒè¯
            input_text = self.processor.decode(inputs['input_ids'][0], skip_special_tokens=False)
            print(f"è¾“å…¥æ–‡æœ¬: '{input_text}'")
            
            # ç”Ÿæˆ
            print("å¼€å§‹ç”Ÿæˆ...")
            start_time = time.time()
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=kwargs.get('max_new_tokens', 50),
                    do_sample=kwargs.get('do_sample', False),
                    temperature=None,
                    top_k=None,
                    top_p=None,
                    pad_token_id=self.processor.tokenizer.eos_token_id,
                    eos_token_id=self.processor.tokenizer.eos_token_id,
                    repetition_penalty=1.0,
                    length_penalty=1.0,
                    early_stopping=True,
                )
            
            generation_time = time.time() - start_time
            print(f"ç”Ÿæˆæ—¶é—´: {generation_time:.3f}s")
            
            # è§£ç å®Œæ•´è¾“å‡º
            full_text = self.processor.decode(outputs[0], skip_special_tokens=True)
            print(f"å®Œæ•´è¾“å‡º: '{full_text}'")
            
            # æå–ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆç§»é™¤è¾“å…¥ promptï¼‰
            if prompt and full_text.startswith(prompt):
                generated_text = full_text[len(prompt):].strip()
            else:
                # å°è¯•æ›´æ™ºèƒ½çš„æå–
                generated_text = full_text
                for possible_prompt in [prompt, prompt.lower(), prompt.capitalize()]:
                    if possible_prompt and generated_text.startswith(possible_prompt):
                        generated_text = generated_text[len(possible_prompt):].strip()
                        break
            
            print(f"ç”Ÿæˆçš„æ–‡æœ¬: '{generated_text}'")
            
            # æ‰“å°ç”Ÿæˆçš„ token ä¿¡æ¯
            generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]
            print(f"ç”Ÿæˆçš„ tokens: {generated_tokens.tolist()}")
            print(f"ç”Ÿæˆ token æ•°é‡: {len(generated_tokens)}")
            
            return generated_text if generated_text else "[NO GENERATION]"
            
        except Exception as e:
            print(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return f"[ERROR: {str(e)}]"

def test_paligemma_hf(args: HFInferArgs):
    """ä½¿ç”¨ HuggingFace PaliGemma æµ‹è¯•é¢„å­˜æ•°æ®"""
    
    # 1) åŠ è½½æ•°æ®
    print(f"=== åŠ è½½ Libero è§‚æµ‹æ•°æ® ===")
    data_file = pathlib.Path(args.data_file)
    if not data_file.exists():
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        print("è¯·å…ˆè¿è¡Œ collect_libero_data.py æ”¶é›†æ•°æ®")
        return
    
    with open(data_file, "rb") as f:
        collected_data = pickle.load(f)
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(collected_data)} ä¸ªä»»åŠ¡çš„æ•°æ®")
    
    # 2) åˆå§‹åŒ– PaliGemma
    try:
        model = HFPaliGemmaInferencer(args.model_name, args.use_gpu)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 3) åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = pathlib.Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # 4) å¯¹æ¯ä¸ªä»»åŠ¡è¿›è¡Œæ¨ç†
    all_results = []
    
    for i, task_data in enumerate(collected_data):
        print(f"\n{'=' * 60}")
        print(f"æ¨ç†ä»»åŠ¡ {i + 1}/{len(collected_data)}")
        print(f"{'=' * 60}")
        
        task_id = task_data["task_id"]
        task_description = task_data["task_description"]
        
        print(f"ä»»åŠ¡ ID: {task_id}")
        print(f"ä»»åŠ¡æè¿°: {task_description}")
        
        # è·å–å¤„ç†åçš„å›¾åƒ
        agentview_img = task_data["images"]["agentview_processed"]
        wrist_img = task_data["images"]["wrist_processed"]
        
        print(f"AgentView å›¾åƒå½¢çŠ¶: {agentview_img.shape}")
        print(f"Wrist å›¾åƒå½¢çŠ¶: {wrist_img.shape}")
        
        # æµ‹è¯•ä¸åŒçš„ prompt
        test_prompts = [
            # f"How to determine whether task '{task_description}' succeeded or failed? Answer: ",
            f"What is on the table? Answer: ",
        ]
        
        task_results = {
            "task_id": task_id,
            "task_description": task_description,
            "agentview_results": [],
            "wrist_results": []
        }
        
        # å¯¹ AgentView å›¾åƒè¿›è¡Œæ¨ç†
        print("\n--- AgentView å›¾åƒæ¨ç† ---")
        for j, prompt in enumerate(test_prompts):
            print(f"\næµ‹è¯• prompt {j+1}: '{prompt}'")
            
            try:
                caption = model.generate_caption(
                    agentview_img, 
                    prompt,
                    max_new_tokens=args.max_new_tokens,
                    do_sample=args.do_sample,
                    temperature=args.temperature,
                    top_k=args.top_k,
                    top_p=args.top_p
                )
                print(f"âœ… ç»“æœ: '{caption}'")
                task_results["agentview_results"].append((prompt, caption, "SUCCESS"))
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
                task_results["agentview_results"].append((prompt, f"[ERROR: {str(e)}]", "ERROR"))
        
        # å¯¹ Wrist å›¾åƒè¿›è¡Œæ¨ç†
        print("\n--- Wrist å›¾åƒæ¨ç† ---")
        for j, prompt in enumerate(test_prompts):
            print(f"\næµ‹è¯• prompt {j+1}: '{prompt}'")
            
            try:
                caption = model.generate_caption(
                    wrist_img, 
                    prompt,
                    max_new_tokens=args.max_new_tokens,
                    do_sample=args.do_sample,
                    temperature=args.temperature,
                    top_k=args.top_k,
                    top_p=args.top_p
                )
                print(f"âœ… ç»“æœ: '{caption}'")
                task_results["wrist_results"].append((prompt, caption, "SUCCESS"))
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
                task_results["wrist_results"].append((prompt, f"[ERROR: {str(e)}]", "ERROR"))
        
        all_results.append(task_results)
        
        # ä¿å­˜ä»»åŠ¡ç»“æœ
        with open(output_dir / f"task_{task_id}_results.txt", "w", encoding='utf-8') as f:
            f.write(f"Task: {task_description}\n")
            f.write(f"Task ID: {task_id}\n")
            f.write("-" * 60 + "\n")
            
            f.write("\nAgentView å›¾åƒç»“æœ:\n")
            f.write("-" * 30 + "\n")
            for prompt, result, status in task_results["agentview_results"]:
                f.write(f"Prompt: '{prompt}'\n")
                f.write(f"Status: {status}\n")
                f.write(f"Result: '{result}'\n")
                f.write("-" * 20 + "\n")
            
            f.write("\nWrist å›¾åƒç»“æœ:\n")
            f.write("-" * 30 + "\n")
            for prompt, result, status in task_results["wrist_results"]:
                f.write(f"Prompt: '{prompt}'\n")
                f.write(f"Status: {status}\n")
                f.write(f"Result: '{result}'\n")
                f.write("-" * 20 + "\n")
        
        # ç»Ÿè®¡æˆåŠŸç‡
        agentview_success = sum(1 for _, _, status in task_results["agentview_results"] if status == "SUCCESS")
        wrist_success = sum(1 for _, _, status in task_results["wrist_results"] if status == "SUCCESS")
        
        print(f"\nğŸ“Š ä»»åŠ¡ {task_id} ç»Ÿè®¡:")
        print(f"   AgentView: {agentview_success}/{len(test_prompts)} æˆåŠŸ")
        print(f"   Wrist: {wrist_success}/{len(test_prompts)} æˆåŠŸ")
    
    # 5) ä¿å­˜å®Œæ•´ç»“æœ
    results_file = output_dir / "all_results.pkl"
    with open(results_file, "wb") as f:
        pickle.dump(all_results, f)
    
    # 6) ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    total_agentview_success = sum(
        sum(1 for _, _, status in task["agentview_results"] if status == "SUCCESS")
        for task in all_results
    )
    total_wrist_success = sum(
        sum(1 for _, _, status in task["wrist_results"] if status == "SUCCESS") 
        for task in all_results
    )
    total_tests = len(all_results) * len(test_prompts)
    
    summary_file = output_dir / "summary.txt"
    with open(summary_file, "w", encoding='utf-8') as f:
        f.write(f"HuggingFace PaliGemma æ¨ç†ç»“æœæ€»ç»“\n")
        f.write(f"=" * 50 + "\n")
        f.write(f"æ¨¡å‹: {args.model_name}\n")
        f.write(f"è®¾å¤‡: {'GPU' if args.use_gpu else 'CPU'}\n")
        f.write(f"æµ‹è¯•ä»»åŠ¡æ•°: {len(all_results)}\n")
        f.write(f"æ¯ä»»åŠ¡æµ‹è¯• prompt æ•°: {len(test_prompts)}\n")
        f.write(f"æ€»æµ‹è¯•æ•°: {total_tests * 2} (AgentView + Wrist)\n")
        f.write(f"\nç”Ÿæˆé…ç½®:\n")
        f.write(f"  - max_new_tokens: {args.max_new_tokens}\n")
        f.write(f"  - do_sample: {args.do_sample}\n")
        f.write(f"  - temperature: {args.temperature}\n")
        f.write(f"  - top_k: {args.top_k}\n")
        f.write(f"  - top_p: {args.top_p}\n")
        f.write(f"\næˆåŠŸç‡ç»Ÿè®¡:\n")
        f.write(f"AgentView å›¾åƒ: {total_agentview_success}/{total_tests} ({total_agentview_success/total_tests*100:.1f}%)\n")
        f.write(f"Wrist å›¾åƒ: {total_wrist_success}/{total_tests} ({total_wrist_success/total_tests*100:.1f}%)\n")
        f.write(f"æ€»ä½“: {total_agentview_success + total_wrist_success}/{total_tests * 2} ({(total_agentview_success + total_wrist_success)/(total_tests * 2)*100:.1f}%)\n")
        
        f.write(f"\nè¯¦ç»†ä»»åŠ¡ç»“æœ:\n")
        for task in all_results:
            agentview_success = sum(1 for _, _, status in task["agentview_results"] if status == "SUCCESS")
            wrist_success = sum(1 for _, _, status in task["wrist_results"] if status == "SUCCESS")
            f.write(f"ä»»åŠ¡ {task['task_id']}: AgentView {agentview_success}/{len(test_prompts)}, Wrist {wrist_success}/{len(test_prompts)}\n")
            f.write(f"  æè¿°: {task['task_description']}\n")
    
    print(f"\nâœ… æ¨ç†å®Œæˆï¼")
    print(f"   - ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"   - æ€»ç»“æŠ¥å‘Š: {summary_file}")
    print(f"   - AgentView æˆåŠŸç‡: {total_agentview_success}/{total_tests} ({total_agentview_success/total_tests*100:.1f}%)")
    print(f"   - Wrist æˆåŠŸç‡: {total_wrist_success}/{total_tests} ({total_wrist_success/total_tests*100:.1f}%)")

if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    tyro.cli(test_paligemma_hf)